{
    "@graph": [
        {
            "@id": "gnd:1047680785",
            "sameAs": "Kaufmann, Paul"
        },
        {
            "@id": "gnd:1107031117",
            "sameAs": "Kramer, Oliver"
        },
        {
            "@id": "gnd:1216138044",
            "sameAs": "Prellberg, Jonas"
        },
        {
            "@id": "gnd:4226127-2",
            "sameAs": "Neuronales Netz"
        },
        {
            "@id": "gnd:4366912-8",
            "sameAs": "Evolutionärer Algorithmus"
        },
        {
            "@id": "https://www.tib.eu/de/suchen/id/TIBKAT%3A1728732700",
            "@type": "bibo:Thesis",
            "P1053": "1 Online-Ressource (viii, 137 Seiten, 1,2 MB)",
            "identifier": [
                "(ppn)1728732700",
                "(firstid)DNB:1216241767"
            ],
            "subject": [
                "(classificationName=ddc)006.31",
                "(classificationName=linseach:mapping)inf",
                "(classificationName=ddc-dbn)004"
            ],
            "title": "Evolving deep neural networks : optimization of weights and architectures",
            "abstract": [
                "Deep neural networks are powerful predictive models that are applied in many different fields of study to great success. However, training a model that generalizes well is a difficult task that requires much computational power, large datasets, and a correct choice of a vast number of hyperparameters. In this thesis, we connect evolutionary approaches to modern, large-scale deep learning to improve the training process in the three mentioned areas. We highlight the influence of the network architecture as an especially important hyperparameter and develop automated architecture search methods. Since network architecture and network weights are closely related, we also propose an evolutionary large-scale network training algorithm. For all algorithms, we place special emphasis on low computational requirements to counteract the cost of the search process. Finally, we are able to reduce the required amount of training data by integrating multi-task learning.",
                "Tiefe neuronale Netze sind mächtige Vorhersagemodelle mit Anwendungen in vielen Disziplinen. Jedoch ist es eine schwierige Aufgabe ein gut generalisierendes Modell zu trainieren, da viel Rechenleistung, große Datensätze und korrekte Hyperparameterwahl erforderlich sind. In dieser Dissertation verbinden wir evolutionäre Ansätze mit modernen, großen neuronalen Netzen um den Trainingsprozess in den drei genannten Bereichen zu verbessern. Wir stellen die Netzwerkarchitektur als besonders wichtigen Hyperparameter heraus und entwickeln automatische Architektursuchalgorithmen. Da Netzwerkarchitektur und -gewichte eng zusammenhängen, schlagen wir einen evolutionären Trainingsalgorithmus für große neuronale Netze vor. Für alle Algorithmen legen wir Wert auf geringen Rechenbedarf, um den erhöhten Anforderungen des Suchprozesses entgegenzuwirken. Abschließend sind wir durch die Integration von multi-task learning in der Lage, die Menge benötigter Trainingsdaten zu verringern."
            ],
            "alternative": "Evolution Tiefer Neuronaler Netze : Optimierung von Gewichten und Architekturen",
            "contributor": [
                "Technische Informationsbibliothek (TIB)",
                {
                    "@id": "gnd:1047680785"
                },
                {
                    "@id": "gnd:1107031117"
                }
            ],
            "creator": "gnd:1216138044",
            "isPartOf": "(collectioncode)GBV-ODiss",
            "issued": "2020",
            "language": "http://id.loc.gov/vocabulary/iso639-1/en",
            "license": "open access",
            "medium": "rda:termList/RDACarrierType/1018",
            "dcterms:subject": [
                {
                    "@id": "gnd:4366912-8"
                },
                {
                    "@id": "gnd:4226127-2"
                }
            ],
            "P60163": "Oldenburg"
        }
    ],
    "@id": "urn:x-arq:DefaultGraphNode",
    "@context": {
        "sameAs": "http://www.w3.org/2002/07/owl#sameAs",
        "subject": "http://purl.org/dc/elements/1.1/subject",
        "contributor": "http://purl.org/dc/terms/contributor",
        "abstract": "http://purl.org/dc/terms/abstract",
        "license": "http://purl.org/dc/terms/license",
        "identifier": "http://purl.org/dc/elements/1.1/identifier",
        "language": {
            "@id": "http://purl.org/dc/terms/language",
            "@type": "@id"
        },
        "isPartOf": "http://purl.org/dc/terms/isPartOf",
        "P60163": "http://www.rdaregistry.info/Elements/u/#P60163",
        "P1053": "http://iflastandards.info/ns/isbd/elements/P1053",
        "medium": {
            "@id": "http://purl.org/dc/terms/medium",
            "@type": "@id"
        },
        "creator": {
            "@id": "http://purl.org/dc/terms/creator",
            "@type": "@id"
        },
        "issued": "http://purl.org/dc/terms/issued",
        "title": "http://purl.org/dc/elements/1.1/title",
        "alternative": "http://purl.org/dc/terms/alternative",
        "umbel": "http://umbel.org/umbel#",
        "rdau": "http://www.rdaregistry.info/Elements/u/#",
        "owl": "http://www.w3.org/2002/07/owl#",
        "dcterms": "http://purl.org/dc/terms/",
        "bibo": "http://purl.org/ontology/bibo/",
        "rdam": "http://www.rdaregistry.info/Elements/m/#",
        "gnd": "http://d-nb.info/gnd/",
        "isbd": "http://iflastandards.info/ns/isbd/elements/",
        "rda": "http://rdvocab.info/",
        "doi": "https://doi.org/"
    }
}