{
    "@graph": [
        {
            "@id": "gnd:1025829506",
            "sameAs": "Klapuri, Anssi"
        },
        {
            "@id": "gnd:1060018942",
            "sameAs": "Abe\u00dfer, Jakob"
        },
        {
            "@id": "gnd:123175844",
            "sameAs": "M\u00fcller, Meinard"
        },
        {
            "@id": "gnd:13763398X",
            "sameAs": "Schuller, Gerald"
        },
        {
            "@id": "gnd:4072803-1",
            "sameAs": "Information Retrieval"
        },
        {
            "@id": "gnd:4120775-0",
            "sameAs": "Musiksignal"
        },
        {
            "@id": "gnd:4191522-7",
            "sameAs": "Tonsignal"
        },
        {
            "@id": "gnd:4261650-5",
            "sameAs": "Elektrobass"
        },
        {
            "@id": "https://www.tib.eu/de/suchen/id/TIBKAT%3A798977809",
            "@type": "bibo:Thesis",
            "P1053": "Online-Ressource (PDF-Datei: XI, 168 S., 4,92 MB)",
            "description": "Ill., graph. Darst.",
            "identifier": [
                "(firstid)GBV:798977809",
                "(ppn)798977809"
            ],
            "publisher": "Univ.-Bibliothek",
            "subject": [
                "(classificationName=linseach:mapping)inf",
                "(classificationName=linseach:mapping)pae",
                "(classificationName=linseach:mapping)elt",
                "(classificationName=ddc-dbn)621.3",
                "(classificationName=ddc-dbn)780",
                "(classificationName=ddc-dbn)004",
                "(classificationName=linseach:mapping)phy"
            ],
            "title": "Automatic transcription of bass guitar tracks applied for music genre classification and sound synthesis",
            "abstract": "Musiksignale bestehen in der Regel aus einer \u00dcberlagerung mehrerer Einzelinstrumente. Die meisten existierenden Algorithmen zur automatischen Transkription und Analyse von Musikaufnahmen im Forschungsfeld des Music Information Retrieval (MIR) versuchen, semantische Information direkt aus diesen gemischten Signalen zu extrahieren. In den letzten Jahren wurde h\u00e4ufig beobachtet, dass die Leistungsf\u00e4higkeit dieser Algorithmen durch die Signal\u00fcberlagerungen und den daraus resultierenden Informationsverlust generell limitiert ist. Ein m\u00f6glicher  L\u00f6sungsansatz besteht darin, mittels Verfahren der Quellentrennung die beteiligten Instrumente vor der Analyse klanglich zu isolieren. Die Leistungsf\u00e4higkeit dieser Algorithmen ist zum aktuellen Stand der Technik jedoch nicht immer ausreichend, um eine sehr gute Trennung der Einzelquellen zu erm\u00f6glichen. In dieser Arbeit werden daher ausschlie\u00dflich isolierte Instrumentalaufnahmen untersucht, die klanglich nicht von anderen Instrumenten \u00fcberlagert sind. Exemplarisch werden anhand der elektrischen Bassgitarre auf die Klangerzeugung dieses Instrumentes hin spezialisierte Analyse- und Klangsynthesealgorithmen entwickelt und evaluiert. Im ersten Teil der vorliegenden Arbeit wird ein Algorithmus vorgestellt, der eine automatische Transkription von Bassgitarrenaufnahmen durchf\u00fchrt. Dabei wird das Audiosignal durch verschiedene Klangereignisse beschrieben, welche den gespielten Noten auf dem Instrument entsprechen. Neben den \u00fcblichen Notenparametern Anfang, Dauer, Lautst\u00e4rke und Tonh\u00f6he werden dabei auch instrumentenspezifische Parameter wie die verwendeten Spieltechniken sowie die Saiten- und Bundlage auf dem Instrument automatisch extrahiert. Evaluationsexperimente anhand zweier neu erstellter Audiodatens\u00e4tze belegen, dass der vorgestellte Transkriptionsalgorithmus auf einem Datensatz von realistischen Bassgitarrenaufnahmen eine h\u00f6here Erkennungsgenauigkeit erreichen kann als drei existierende Algorithmen aus dem Stand der Technik. Die Sch\u00e4tzung der instrumentenspezifischen Parameter kann insbesondere f\u00fcr isolierte Einzelnoten mit einer hohen G\u00fcte durchgef\u00fchrt werden. Im zweiten Teil der Arbeit wird untersucht, wie aus einer Notendarstellung typischer sich wiederholender Basslinien auf das Musikgenre geschlossen werden kann. Dabei werden Audiomerkmale extrahiert, welche verschiedene tonale, rhythmische, und strukturelle Eigenschaften von Basslinien quantitativ beschreiben. Mit Hilfe eines neu erstellten Datensatzes von 520 typischen Basslinien aus 13 verschiedenen Musikgenres wurden drei verschiedene Ans\u00e4tze f\u00fcr die automatische Genreklassifikation verglichen. Dabei zeigte sich, dass mit Hilfe eines regelbasierten Klassifikationsverfahrens nur Anhand der Analyse der Basslinie eines Musikst\u00fcckes bereits eine mittlere Erkennungsrate von 64,8 % erreicht werden konnte. Die Re-synthese der originalen Bassspuren basierend auf den extrahierten Notenparametern wird im dritten Teil der Arbeit untersucht. Dabei wird ein neuer Audiosynthesealgorithmus vorgestellt, der basierend auf dem Prinzip des Physical Modeling verschiedene Aspekte der f\u00fcr die Bassgitarre charakteristische Klangerzeugung wie Saitenanregung, D\u00e4mpfung, Kollision zwischen Saite und Bund sowie dem Tonabnehmerverhalten nachbildet. Weiterhin wird ein parametrischerAudiokodierungsansatz diskutiert, der es erlaubt, Bassgitarrenspuren nur anhand der ermittelten notenweisen Parameter zu \u00fcbertragen um sie auf Dekoderseite wieder zu resynthetisieren. Die Ergebnisse mehrerer H\u00f6test belegen, dass der vorgeschlagene Synthesealgorithmus eine Re- Synthese von Bassgitarrenaufnahmen mit einer besseren Klangqualit\u00e4t erm\u00f6glicht als die \u00dcbertragung der Audiodaten mit existierenden Audiokodierungsverfahren, die auf sehr geringe Bitraten ein gestellt sind.",
            "contributor": [
                "gnd:123175844",
                "gnd:1025829506",
                "gnd:13763398X"
            ],
            "dcterms:contributor": "Technische Informationsbibliothek (TIB)",
            "creator": "gnd:1060018942",
            "isPartOf": "(collectioncode)GBV-ODiss",
            "issued": "2014",
            "language": "http://id.loc.gov/vocabulary/iso639-1/en",
            "license": "open access",
            "medium": "rda:termList/RDACarrierType/1018",
            "dcterms:subject": [
                {
                    "@id": "gnd:4120775-0"
                },
                {
                    "@id": "gnd:4191522-7"
                }
            ]
        }
    ],
    "@id": "urn:x-arq:DefaultGraphNode",
    "@context": {
        "creator": {
            "@id": "http://purl.org/dc/terms/creator",
            "@type": "@id"
        },
        "language": {
            "@id": "http://purl.org/dc/terms/language",
            "@type": "@id"
        },
        "contributor": {
            "@id": "http://purl.org/dc/terms/contributor",
            "@type": "@id"
        },
        "subject": "http://purl.org/dc/elements/1.1/subject",
        "medium": {
            "@id": "http://purl.org/dc/terms/medium",
            "@type": "@id"
        },
        "title": "http://purl.org/dc/elements/1.1/title",
        "license": "http://purl.org/dc/terms/license",
        "identifier": "http://purl.org/dc/elements/1.1/identifier",
        "issued": "http://purl.org/dc/terms/issued",
        "isPartOf": "http://purl.org/dc/terms/isPartOf",
        "abstract": "http://purl.org/dc/terms/abstract",
        "P1053": "http://iflastandards.info/ns/isbd/elements/P1053",
        "description": "http://purl.org/dc/elements/1.1/description",
        "publisher": "http://purl.org/dc/elements/1.1/publisher",
        "sameAs": "http://www.w3.org/2002/07/owl#sameAs",
        "umbel": "http://umbel.org/umbel#",
        "rdau": "http://www.rdaregistry.info/Elements/u/#",
        "owl": "http://www.w3.org/2002/07/owl#",
        "dcterms": "http://purl.org/dc/terms/",
        "bibo": "http://purl.org/ontology/bibo/",
        "rdam": "http://www.rdaregistry.info/Elements/m/#",
        "gnd": "http://d-nb.info/gnd/",
        "isbd": "http://iflastandards.info/ns/isbd/elements/",
        "rda": "http://rdvocab.info/",
        "doi": "https://doi.org/"
    }
}