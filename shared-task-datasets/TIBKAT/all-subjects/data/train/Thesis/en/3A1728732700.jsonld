{
    "@graph": [
        {
            "@id": "gnd:1047680785",
            "sameAs": "Kaufmann, Paul"
        },
        {
            "@id": "gnd:1107031117",
            "sameAs": "Kramer, Oliver"
        },
        {
            "@id": "gnd:1216138044",
            "sameAs": "Prellberg, Jonas"
        },
        {
            "@id": "gnd:4226127-2",
            "sameAs": "Neuronales Netz"
        },
        {
            "@id": "gnd:4366912-8",
            "sameAs": "Evolution\u00e4rer Algorithmus"
        },
        {
            "@id": "https://www.tib.eu/de/suchen/id/TIBKAT%3A1728732700",
            "@type": "bibo:Thesis",
            "P1053": "1 Online-Ressource (viii, 137 Seiten, 1,2 MB)",
            "identifier": [
                "(ppn)1728732700",
                "(firstid)DNB:1216241767"
            ],
            "subject": [
                "(classificationName=ddc)006.31",
                "(classificationName=linseach:mapping)inf",
                "(classificationName=ddc-dbn)004"
            ],
            "title": "Evolving deep neural networks : optimization of weights and architectures",
            "abstract": [
                "Deep neural networks are powerful predictive models that are applied in many different fields of study to great success. However, training a model that generalizes well is a difficult task that requires much computational power, large datasets, and a correct choice of a vast number of hyperparameters. In this thesis, we connect evolutionary approaches to modern, large-scale deep learning to improve the training process in the three mentioned areas. We highlight the influence of the network architecture as an especially important hyperparameter and develop automated architecture search methods. Since network architecture and network weights are closely related, we also propose an evolutionary large-scale network training algorithm. For all algorithms, we place special emphasis on low computational requirements to counteract the cost of the search process. Finally, we are able to reduce the required amount of training data by integrating multi-task learning.",
                "Tiefe neuronale Netze sind m\u00e4chtige Vorhersagemodelle mit Anwendungen in vielen Disziplinen. Jedoch ist es eine schwierige Aufgabe ein gut generalisierendes Modell zu trainieren, da viel Rechenleistung, gro\u00dfe Datens\u00e4tze und korrekte Hyperparameterwahl erforderlich sind. In dieser Dissertation verbinden wir evolution\u00e4re Ans\u00e4tze mit modernen, gro\u00dfen neuronalen Netzen um den Trainingsprozess in den drei genannten Bereichen zu verbessern. Wir stellen die Netzwerkarchitektur als besonders wichtigen Hyperparameter heraus und entwickeln automatische Architektursuchalgorithmen. Da Netzwerkarchitektur und -gewichte eng zusammenh\u00e4ngen, schlagen wir einen evolution\u00e4ren Trainingsalgorithmus f\u00fcr gro\u00dfe neuronale Netze vor. F\u00fcr alle Algorithmen legen wir Wert auf geringen Rechenbedarf, um den erh\u00f6hten Anforderungen des Suchprozesses entgegenzuwirken. Abschlie\u00dfend sind wir durch die Integration von multi-task learning in der Lage, die Menge ben\u00f6tigter Trainingsdaten zu verringern."
            ],
            "alternative": "Evolution Tiefer Neuronaler Netze : Optimierung von Gewichten und Architekturen",
            "contributor": [
                "Technische Informationsbibliothek (TIB)",
                {
                    "@id": "gnd:1047680785"
                },
                {
                    "@id": "gnd:1107031117"
                }
            ],
            "creator": "gnd:1216138044",
            "isPartOf": "(collectioncode)GBV-ODiss",
            "issued": "2020",
            "language": "http://id.loc.gov/vocabulary/iso639-1/en",
            "license": "open access",
            "medium": "rda:termList/RDACarrierType/1018",
            "dcterms:subject": [
                {
                    "@id": "gnd:4366912-8"
                },
                {
                    "@id": "gnd:4226127-2"
                }
            ],
            "P60163": "Oldenburg"
        }
    ],
    "@id": "urn:x-arq:DefaultGraphNode",
    "@context": {
        "sameAs": "http://www.w3.org/2002/07/owl#sameAs",
        "subject": "http://purl.org/dc/elements/1.1/subject",
        "contributor": "http://purl.org/dc/terms/contributor",
        "abstract": "http://purl.org/dc/terms/abstract",
        "license": "http://purl.org/dc/terms/license",
        "identifier": "http://purl.org/dc/elements/1.1/identifier",
        "language": {
            "@id": "http://purl.org/dc/terms/language",
            "@type": "@id"
        },
        "isPartOf": "http://purl.org/dc/terms/isPartOf",
        "P60163": "http://www.rdaregistry.info/Elements/u/#P60163",
        "P1053": "http://iflastandards.info/ns/isbd/elements/P1053",
        "medium": {
            "@id": "http://purl.org/dc/terms/medium",
            "@type": "@id"
        },
        "creator": {
            "@id": "http://purl.org/dc/terms/creator",
            "@type": "@id"
        },
        "issued": "http://purl.org/dc/terms/issued",
        "title": "http://purl.org/dc/elements/1.1/title",
        "alternative": "http://purl.org/dc/terms/alternative",
        "umbel": "http://umbel.org/umbel#",
        "rdau": "http://www.rdaregistry.info/Elements/u/#",
        "owl": "http://www.w3.org/2002/07/owl#",
        "dcterms": "http://purl.org/dc/terms/",
        "bibo": "http://purl.org/ontology/bibo/",
        "rdam": "http://www.rdaregistry.info/Elements/m/#",
        "gnd": "http://d-nb.info/gnd/",
        "isbd": "http://iflastandards.info/ns/isbd/elements/",
        "rda": "http://rdvocab.info/",
        "doi": "https://doi.org/"
    }
}