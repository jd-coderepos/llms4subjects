{
    "@graph": [
        {
            "@id": "gnd:1148296905",
            "sameAs": "Naseer, Tayyab"
        },
        {
            "@id": "gnd:4129594-8",
            "sameAs": "Maschinelles Sehen"
        },
        {
            "@id": "gnd:4261462-4",
            "sameAs": "Robotik"
        },
        {
            "@id": "gnd:7842968-7",
            "sameAs": "SLAM-Verfahren"
        },
        {
            "@id": "https://www.tib.eu/de/suchen/id/TIBKAT%3A1009134868",
            "@type": "bibo:Thesis",
            "P1053": "1 Online-Ressource (xv, 154 Seiten)",
            "description": "Illustrationen, Diagramme",
            "identifier": [
                "(firstid)BSZ:496206982",
                "(doi)10.6094/UNIFR/13894",
                "(ppn)1009134868"
            ],
            "title": "Robust monocular camera-based localization under challenging perceptual conditions",
            "abstract": "Abstract: Visual localization of autonomous vehicles is imperative to their safe navigation in particular scenarios. The paradigm of monocular camera localization encompasses broad range of applications like place recognition, loop closure detections within a SLAM framework, 6-DoF metric camera pose estimation and augmented reality. To achieve accurate and reliable pose estimates, camera-based localization approaches need to cope with wide variety of challenging perceptual conditions. These methods should be robust to variation in illumination due to day-night changes, different weather conditions and seasonal variations. An autonomous robot should be able to cope with all such variations that enables it to operate over long periods of time.<br>In this thesis, we primarily focus on novel robust methods for long term camera-based visual localization that cope with wide variety of viewing conditions. We highlight the shortcomings of state-of-the-art feature-based methods for monocular cameras and present novel approaches to achieve reliable localization. Although, cameras provide an cost effective solution for autonomous navigation, image data is highly vulnerable to environmental conditions. The rich texture and color information in the images vary drastically over long periods of time. In this work, we develop distinctive and repeatable feature descriptions of images that allow robots to localize under a great spectrum of perceptual conditions. We also develop novel methods that leverage semantics of the environment to imitate human-like behavior in autonomous systems during localization. These methods also scale well with the large map sizes. <br>At first, we discuss both the advantages and shortcomings of the visual information from the cameras for longterm localization. We identify the drawbacks of traditional sparse feature-based methods for longterm localization and propose dense feature descriptions to cope with the pitfalls. We show that by leveraging the sequential nature of the recorded and live data stream from the robot within a network flow graph framework enables to remove false loop closure hypothesis and achieve more precise location estimates. This enables the robot to achieve robust topological localization in a place recognition framework. Although, our approach can handle natural driving maneuvers, the restricted graph connectivity limits its advantages for short, fragmented and more complex trajectories. We discuss a novel approach to h ...",
            "contributor": "Technische Informationsbibliothek (TIB)",
            "creator": "gnd:1148296905",
            "isPartOf": "(collectioncode)GBV-ODiss",
            "issued": "2017",
            "language": "http://id.loc.gov/vocabulary/iso639-1/en",
            "license": "open access",
            "medium": "rda:termList/RDACarrierType/1018",
            "isLike": "doi:10.6094/UNIFR/13894",
            "P60163": "Freiburg"
        }
    ],
    "@id": "urn:x-arq:DefaultGraphNode",
    "@context": {
        "sameAs": "http://www.w3.org/2002/07/owl#sameAs",
        "P60163": "http://www.rdaregistry.info/Elements/u/#P60163",
        "isPartOf": "http://purl.org/dc/terms/isPartOf",
        "creator": {
            "@id": "http://purl.org/dc/terms/creator",
            "@type": "@id"
        },
        "medium": {
            "@id": "http://purl.org/dc/terms/medium",
            "@type": "@id"
        },
        "subject": "http://purl.org/dc/elements/1.1/subject",
        "license": "http://purl.org/dc/terms/license",
        "description": "http://purl.org/dc/elements/1.1/description",
        "identifier": "http://purl.org/dc/elements/1.1/identifier",
        "P1053": "http://iflastandards.info/ns/isbd/elements/P1053",
        "isLike": {
            "@id": "http://umbel.org/umbel#isLike",
            "@type": "@id"
        },
        "issued": "http://purl.org/dc/terms/issued",
        "language": {
            "@id": "http://purl.org/dc/terms/language",
            "@type": "@id"
        },
        "abstract": "http://purl.org/dc/terms/abstract",
        "contributor": "http://purl.org/dc/terms/contributor",
        "title": "http://purl.org/dc/elements/1.1/title",
        "umbel": "http://umbel.org/umbel#",
        "rdau": "http://www.rdaregistry.info/Elements/u/#",
        "owl": "http://www.w3.org/2002/07/owl#",
        "dcterms": "http://purl.org/dc/terms/",
        "bibo": "http://purl.org/ontology/bibo/",
        "rdam": "http://www.rdaregistry.info/Elements/m/#",
        "gnd": "http://d-nb.info/gnd/",
        "isbd": "http://iflastandards.info/ns/isbd/elements/",
        "rda": "http://rdvocab.info/",
        "doi": "https://doi.org/"
    }
}